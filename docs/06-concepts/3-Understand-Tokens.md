---
title: "Tokenization"
slug: /tokenization
---

これまでのレッスンで「トークン」について何度か言及しましたが、それが何であり、なぜ重要なのかを説明していませんでした。今それについて説明しましょう。  
   
## トークン化とは何ですか？  
   
OpenAIの自然言語モデルは、単語や文字をテキストの単位として操作するのではなく、その中間のものを使用します。それが**トークン**です。[定義](https://platform.openai.com/docs/introduction/tokens)によれば、トークンは大規模な言語トレーニングデータセット内で_一般的に発生する文字のシーケンス_を表すテキストの「チャンク」です。  
- トークンは単一の文字、一部の単語、または全体の単語であることがあります。  
- 多くの一般的な単語は1つのトークンで表されます。  
- あまり一般的でない単語は複数のトークンで表されます。  
   
**トークン化**とは、テキストデータ（例えば、「プロンプト」）がトークンのシーケンスに_分解_されるプロセスです。その後、モデルはテキストの「完了」のためにシーケンス内の次のトークンを生成できます。このレッスンの後半でトークン化の具体例を見ていきます。  
   
## トークンはどのように使用されるのですか？  
   
入力プロンプトが与えられると、自然言語モデルはトークンを1つずつ生成して完了します。ただし、生成されるトークンは決定的ではありません。各ステップで、モデルはすべての可能なトークンとそれに関連する重みのリストを出力します。APIはこのリストから1つのトークンをサンプリングし、重みが大きいトークンが他のトークンよりも選択される可能性が高くなります。

![Explanation of tokens used](../images/llm-002.png)

その後、モデルはそのトークンをプロンプトに追加し、「最大トークン数」制限（コンテキストウィンドウ）に達するまで、または特別な「ストップトークン」が生成されてトークン生成が停止されるまで、このプロセスを繰り返します。（このプロセスの詳細については、Beatriz Stollnitzによる[ブログ記事](https://bea.stollnitz.com/blog/how-gpt-works/)をご覧ください。）これがモデルが一つ以上の単語を生成する方法であり、なぜその生成が呼び出しごとに変わるかの理由です。  
   
## なぜ重要なのか？  
   
トークン化がなぜ重要かを理解するためには、デプロイされたモデルの二つの側面、_トークン制限_と_トークン料金_について考える必要があります。  
   
**トークン制限**。すべてのモデルには、単一のリクエストで処理できる最大トークン数として定義されたコンテキストウィンドウがあります。例えば、古いgpt-3.5-turboモデルは、各リクエストに対して4Kトークンの制限（コンテキスト）があります。トークン制限は_プロンプトと完了の間で共有されます_。完了が次のトークンを生成するためにプロンプトに追加されるため、単一のリクエストのために両方を総コンテキストウィンドウ内に収めることが必要になります。  
   
**トークン料金**。他のAPIと同様に、モデルのデプロイメント使用には、モデルの種類とバージョンに基づいたコストがかかります。現在、モデルの料金は使用されたトークン数に結びついており、モデルの種類やバージョンごとに異なる価格ポイントが可能です。  
   
以下の表は、Azure OpenAIモデルのコンテキストウィンドウ（最大トークン数）とモデル料金（1K単位で請求）を示しています。

![Token Pricing](../images/aoia-pricing-tokens.png)


新しいモデル、例えばgpt-4-32kのようなモデルは、最大32,768トークンまでのはるかに大きなトークン制限を持っていることに注目してください。これにより、より長い完了やはるかに大きなプロンプトが可能になります。これは、後で見るようにプロンプトエンジニアリングに特に有用です。ただし、使用コストもそれに応じて高くなることに注意が必要です。プロンプトエンジニアリング技術は、応答の質を損なうことなくトークン使用コストを最小限に抑えるプロンプトを作成することで、コスト効率を改善するのにも役立ちます。  
   
## OpenAIトークナイザーツール  
   
実際のテキストでトークン化がどのように機能するかをよりよく理解したいですか？[**OpenAIトークナイザー**](https://platform.openai.com/tokenizer)を使用してみてください。これは、トークン化を視覚化し、与えられたテキストデータのトークン総数を表示する無料のオンラインツールです。  
   
[🔖 詳細を学ぶ:](https://help.openai.com/articles/4936856-what-are-tokens-and-how-to-count-them)  
   
### 例を試してみる  
   
サイトを訪れ、「例を表示」をクリックして、下のように実際に動作する様子を確認してください。各色分けされたセグメントは単一のトークンを表し、トークン総数は下に表示されます（**57トークン**）。「1234567890」と「underlying」が同じ文字数であるにもかかわらず、前者は4トークン、後者は1トークンであることに注目してください。また、句読点（「:」「.」）がそれぞれ1トークンを占め、プロンプトのトークン制限に影響を与えることも観察してください。

![Image of tokenizer example](../images/tokenizer-example.png)

### 練習を試してみる  
   
:::tip YOUR TURN  
[**https://platform.openai.com/tokenizer**](https://platform.openai.com/tokenizer)にアクセスしてください。各練習の前にツールをクリアしてください。練習テキストをトークナイザーに入力し、出力を観察してください - インタラクティブに更新されるはずです。  
:::  
   
**練習1:** 一般的な単語である「apple」は1つのトークンだけを必要とします。  
```apple```  
   
**練習2:** 単語「blueberries」は2つのトークン「blue」と「berries」を必要とします。  
```blueberries```  
   
**練習3:** 固有名詞は（一般的でない限り）通常複数のトークンを必要とします  
```Skarsgård```  
   
このトークン表現により、AIモデルは辞書にない単語を生成することができ、文字ごとにテキストを生成する必要がなくなります（これにより簡単に無意味な文字列が生成される可能性があります）。**他の単語やフレーズを試して直感を養いましょう。**